<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html class="gr__ai_stanford_edu"><head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-93062604-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-93062604-2');
  </script>
</head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */

    a {
      color: #1772d0;
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }

    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }

    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }

    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }

    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      font-weight: 700
    }

    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }

    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }

    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <style type="text/css">
    #sectiontohide {
      padding: 20px;
      background: #f0f0f0;
      width: 400px;
    }
  </style>
  <script type="text/javascript">
    function toggle_div_fun(id) {

      var divelement = document.getElementById(id);

      if (divelement.style.display == 'none')
        divelement.style.display = 'block';
      else
        divelement.style.display = 'none';
    }
  </script>
  <link rel="icon" type="image/png" href="./Shengju_files/cuhk.png">
  <title>Shengju Qian</title>

  <link href="./Shengju_files/css" rel="stylesheet" type="text/css">
<body data-gr-c-s-loaded="true">
  <table width="1050" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody><tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody><tr>
            <td width="80%" valign="middle">
              <p align="center">
                <name>Shengju Qian</name>
              </p>
              <p>
                I am a PhD student at <a href="https://www.cse.cuhk.edu.hk/en/">The Chinese University of Hong Kong</a>, supervised by <a href="https://jiaya.me/">Prof. Jiaya Jia</a>.
              </p>
              <p>
                Prior to that, I received my Bachelor degree from the University of Electronic Science and Technology of China. I've spent wonderful time with my internship at Amazon AI, Tencent Youtu X-Lab, and SenseTime Research.
              </p>
              <p>
                My research interests lie primarily in computer vision and deep learning,
                particularly focusing on representaion learning and network designs, including self-supervised learning, transformers for vision, and generative models for face/body understanding.
              </p>
              <p align="center">
                <a href="mailto:sjqian@cse.cuhk.edu.hk"> Email </a> /
                <a href="https://www.linkedin.com/in/shengju-qian-0a3605112/"> LinkedIn </a> /
                <a href="https://scholar.google.com/citations?user=QNnWmasAAAAJ&hl=en"> Google Scholar </a> /
                <a href="https://github.com/thesouthfrog"> Github </a>
              </p>
            </td>
            <td width="20%">
              <img src="./Shengju_files/shengju.jpg" width="90" height="90">
              </div>
              <div id="profile">Me in 2003</div>
              </div>
            </td>
          </tr>
        </tbody></table>

        <!--##################### News ####################-->
        <br><br>
         <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody><tr>
              <heading>News</heading>
          </tr>
        </tbody></table>

         <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody><tr>
            <p>
            <ul>
                <li>2021-10: One paper is accpeted to NeurIPS 2021. We explore anti-aliasing designs for vision transformers.</li>
            </ul>
            </p>
          </tr>
        </tbody></table>
        </br>





                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                  <tbody><tr>
                    <td width="100%" valign="middle">
                      <heading>Publications</heading>
                    </td>
                  </tr>
                </tbody></table>

                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                  <tbody><tr>
                    <td width="25%" align="center">
                      <img src="./Shengju_files/neurips_arm.png" alt="dise" width="240" height="96">
                    </td>
                    <td width="75%" valign="top">
                      <p>
                        <papertitle>Blending Anti-Aliasing into Vision Transformer</papertitle>
                        <br>
                         <strong><u>Shengju Qian</u></strong>, Hao Shao, <a href="https://bryanyzhu.github.io/">Yi Zhu</a>, <a href="https://scholar.google.com/citations?user=Z_WrhK8AAAAJ&hl=en">Mu Li</a>, <a href="https://jiaya.me/">Jiaya Jia</a>
                        <br>
                        <em>Neural Information Processing Systems (NeurIPS) </em>, 2021
                        <br>
                        <a href="./Shengju_files/neurips_arm.pdf">[pdf]</a> <a href="https://github.com/amazon-research/anti-aliasing-transformer">[code]</a> <a href="./Shengju_files/bibtex/neurips2020arm_bib.txt">[bibtex]</a>
                        <br>
                        </p><p>
                          We explore anti-aliasing strategies for vision transformers to mitigate the drawback of 'patch-wise' tokenization.
                        </p>
                    </td>
                  </tr>

                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                  <tbody><tr>
                    <td width="25%" align="center">
                      <img src="./Shengju_files/TIN.png" alt="dise" width="200" height="100">
                    </td>
                    <td width="75%" valign="top">
                      <p>
                        <papertitle>Temporal Interlacing Network</papertitle>
                        <br>
                         Hao Shao, <strong><u>Shengju Qian</u></strong>, <a href="https://liuyu.us/">Yu Liu</a>
                        <br>
                        <em>AAAI Conference on Artificial Intelligence (AAAI)</em>, 2020
                        <br>
                        <a href="./Shengju_files/TIN.pdf">[pdf]</a> <a href="https://github.com/Sense-X/X-Temporal">[code]</a> <a href="./Shengju_files/bibtex/aaai2020tin_bib.txt">[bibtex]</a>
                        <br>
                        </p><p>
                          We propose an efficient and accurate framework for video recognition with TIN, which has a strong capability of temporal modeling.
                        </p>
                    </td>
                  </tr>

                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                  <tbody><tr>
                    <td width="25%" align="center">
                      <img src="./Shengju_files/iccv2019style.png" alt="dise" width="200" height="80">
                    </td>
                    <td width="75%" valign="top">
                      <p>
                        <papertitle>Aggregation via Separation: Boosting Facial Landmark Detector with Semi-Supervised Style Translation</papertitle>
                        <br>
                        <strong><u>Shengju Qian</u></strong>, <a href="https://keqiangsun.github.io/">Keqiang Sun</a>, <a href="https://wywu.github.io/">Wayne Wu</a>, <a href="https://scholar.google.com/citations?user=AerkT0YAAAAJ&hl=en">Chen Qian</a>, <a href="https://jiaya.me/">Jiaya Jia</a>
                        <br>
                        <em>IEEE International Conference on Computer Vision (ICCV)</em>, 2019
                        <br>
                        <a href="https://arxiv.org/pdf/1908.06440.pdf">[pdf]</a> <a href="https://github.com/TheSouthFrog/stylealign">[code]</a> <a href="./Shengju_files/bibtex/iccv2019style_bib.txt">[bibtex]</a>
                        <br>
                        </p><p>
                          We propose a semi-supervised learning algorithm for boosting facial landmark detectors.
                        </p>
                    </td>
                  </tr>

                </tbody></table>

                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                  <tbody><tr>
                    <td width="25%" align="center">
                      <img src="./Shengju_files/iccv2019face.jpg" alt="makeface" width="210" height="140">
                    </td>
                    <td width="75%" valign="top">
                      <p>
                        <papertitle>Make a Face: Towards Arbitrary High Fidelity Face Manipulation</papertitle>
                        <br>
                        <strong><u>Shengju Qian</u></strong>, <a href="https://kwanyeelin.github.io/">Kwan-Yee Lin</a>, <a href="https://wywu.github.io/">Wayne Wu</a>, <a href="https://scholar.google.com/citations?user=AerkT0YAAAAJ&hl=en">Chen Qian</a>, <a href="https://scholar.google.com/citations?user=ayrg9AUAAAAJ&hl=en">Ran He</a> et al
                        <br>
                        <em>IEEE International Conference on Computer Vision (ICCV)</em>, 2019
                        <br>
                        <a href="https://arxiv.org/pdf/1908.07191.pdf">[pdf]</a> [project page] <a href="./Shengju_files/bibtex/iccv2019face_bib.txt">[bibtex]</a>
                        <br>
                        </p><p>
                          We propose Addtive Focal Variational Autoencoder(AF-VAE) for high fidelity face manipulation, which is robust to arbitrary shape variation.
                        </p>
                    </td>
                  </tr>

                </tbody></table>

                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
                  <tbody><tr>
                    <td width="25%" align="center">
                      <img src="./Shengju_files/nipsw2018cvae.png" alt="cvae" width="160" height="60">
                    </td>
                    <td width="75%" valign="top">
                      <p>
                        <papertitle>Extending the Capacity of CVAE for Face Synthesis and Modeling</papertitle>
                        <br>
                        <strong><u>Shengju Qian</u></strong>, <a href="https://wywu.github.io/">Wayne Wu</a>, Yangxiaokang Liu, Beier Zhu, <a href="https://scholar.google.com/citations?user=oqYL6fQAAAAJ&hl=en">Fumin Shen</a>
                        <br>
                        <em>NeurIPS Workshop on Relational Representation Learning Workshop (NIPSW)</em>, 2018
                        <br>
                        <a href="https://r2learning.github.io/assets/papers/CameraReadySubmission%2011.pdf">[pdf]</a> <a href="./Shengju_files/bibtex/nipsw2018cvae_bib.txt">[bibtex]</a>
                        <br>
                        </p><p>
                          Presenting a practical methodology that can boost CVAE's representation power on face synthesis and modeling.
                        </p>
                    </td>
                  </tr>



                  <br><br>
                  <table width="100%" align="left" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                      <heading>Experience</heading>
                    </tr>
                  </table>
                  <table width="100%" align="center" border="0" cellpadding="10">
                    <tbody><tr>
                      <td width="75%" valign="top">
                        <strong>June. 2020 - Present</strong>, <i><b>Amazon AI</b></i> Mentor: Yi Zhu, Mu Li. <br>
                      </td>
                    </tr>
                  </tbody></table>
                  <table width="100%" align="center" border="0" cellpadding="10">
                    <tbody><tr>
                      <td width="75%" valign="top">
                        <strong>Dec. 2018 - Apr.2019</strong>, <i><b>Tencent Youtu, X-Lab</b></i>  Mentor: Xiaoyong Shen. <br>
                      </td>
                    </tr>
                  </tbody></table>

                  <table width="100%" align="center" border="0" cellpadding="10">
                    <tbody><tr>
                      <td width="75%" valign="top">
                        <strong>Mar. 2018 - Dec.2018</strong>, <i><b>SenseTime Research</b></i> Mentor: Wayne Wu, Chen Qian. <br>
                      </td>
                    </tr>
                  </tbody></table>

                </br>

                  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tbody><tr>
                      <td>
                        <heading>Services</heading>
                      </td>
                    </tr>
                  </tbody></table>
                  <ul>
                  <li>
                    <p>Conference Reviewer: CVPR 2020-2021, ECCV 2020, AAAI 2020-2021, ICML 2021, NeurIPS 2021, ICLR 2021</p>
                  </li>
                  <li>
                    <p>Journal Reviewer:Transactions on Image Processing(TIP), Transactions on Pattern Analysis and Machine Intelligence(TPAMI) </p>
                  </li>
                  <li>
                    <p>Teaching: CSCI 2040B: Introduction to Python, ENGG 2780Aï¼š Statistics for Engineers</p>
                  </li>
                </ul>


                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                  <tbody><tr>
                    <td>
                      <heading>Honors and Awards</heading>
                    </td>
                  </tr>
                </tbody></table>
                <ul>
                  <li>
                    <p>Postgraduate Studentship, CUHK, 2019-2023</p>
                  </li>
                  <li>
                    <p>Outstanding Reviewer, ECCV, 2020</p>
                  </li>
                  <li>
                    <p>Outstanding Bachelor Thesis, UESTC, 2019</p>
                  </li>
                  <li>
                    <p>First Runner-up, ECCV VisDA Challenge(Rank 2nd), 2018</p>
                  </li>
                  <li>
                    <p>First Runner-up, CVPR Webvision Challenge(Rank 2nd), 2018</p>
                  </li>
                  <li>
                    <p>Special Prize Scholarship, UESTC, 2018</p>
                  </li>
                </ul>

                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                  <tbody><tr>
                    <td>
                      <heading>Misc</heading>
                    </td>
                  </tr>
                </tbody></table>
                <ul>
                  <li>
                    <p>I am a beginner for Triathlon.</p>
                  </li>
                  <li>
                    <p>Used to cycle long distances(such as Qinghai-Tibet: around 2000KM).</p>
                    <div align="center"><img src="Shengju_files/bike.jpg" height="200"/></div>
                    <div align="center">Hoh Xil, 2016</div>
                  </li>
                </ul>

        <br>
        <!-- Footer ================================================== -->
        <hr>

        <footer class="footer">
          <div class="container">
            <p>
              &nbsp;&nbsp;&nbsp;Updated Nov. 2021</p>
            <p></p>
            <p align="right">
              <a href="https://jonbarron.info/"> Page Template </a>
              <a class="pull-right" href="#">
                <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=n&d=CU8xe9Z-Wjn88XyLXx5PAXRa-KFgVTNSp-e1SQErc4g"></script>
              </a>
            </p>
          </div>
        </footer>
      </td>
    </tr>
  </tbody></table>


<div class="jvectormap-tip"></div></body></html>
